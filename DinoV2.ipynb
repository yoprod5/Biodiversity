{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow General GEOCLEF Challenge by YOUME "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example code of how tu use provided data providers and dataset to create a pytorch dataset based on patches data.\n",
    "\n",
    "\n",
    "## Imports\n",
    "Importing the providers and dataset objects:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Providers\n",
    "\n",
    "Define the list of providers: Providers are datareaders specific to a type of patch. ``JpegPatchProvider`` allows to find and load a jpeg patches from a 'patchID', ``MultipleRasterPatchProvider`` and ``RasterPatchProvider`` allow to read in one or multiple rasters to extract patches given coordinates. You need to configure the list of providers for the data you want to include in the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.GLC23PatchesProviders import MultipleRasterPatchProvider, RasterPatchProvider, JpegPatchProvider\n",
    "from data.GLC23Datasets import PatchesDataset, PatchesDatasetMultiLabel, PatchesDatasetBlind\n",
    "\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from transformers import AutoImageProcessor, Dinov2ForImageClassification, Dinov2Model\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path = '/Users/yoprod/Desktop/MesRecherches/GeoClef/' # root path of the data\n",
    "#data_path2 = 'data/zenith/share/GLC/patches/\n",
    "# root path of the data\n",
    "# configure providers\n",
    "p_rgb = JpegPatchProvider(data_path+'SatelliteImages/', select=['red', 'green', 'blue'], normalize=False) # take all sentinel imagery layers (r,g,b,nir = 4 layers)\n",
    "\n",
    "# create dataset\n",
    "#dataset = PatchesDataset(\n",
    "#    occurrences=data_path+'Presence_only_occurrences/Presences_only_train_sample.csv',\n",
    "#    providers=(p_hfp_d, p_bioclim, p_hfp_s, p_rgb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Construct the dataset object given the list of providers and the occurrences csv file: The ``PatchesDataset`` class inherits from the ``Dataset`` class of ``torch.utils.data``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "\n",
    "evalset = PatchesDatasetBlind(\n",
    "    occurrences=data_path+'GLC/test_blind.csv',\n",
    "    providers=(p_rgb))\n",
    "\n",
    "\n",
    "# Initialize the dataloaders for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch dataloader Images\n",
    "\n",
    "The ``PatchesDataset`` can be then wrapped in a torch dataloader:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dino V2 Model Building\n",
    "In this section we try to run the DinoV2 Model in Images of Species From Satellites and at the end extract information from Emnbedding vector for building a Alpha and Beta Diversity Map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, Dinov2ForImageClassification\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "#dataset = load_dataset(\"huggingface/cats-image\")\n",
    "#image = dataset[\"test\"][\"image\"][0]\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
    "model = Dinov2ForImageClassification.from_pretrained(\"facebook/dinov2-base\")\n",
    "\n",
    "for image in \n",
    "\n",
    "    inputs = image_processor(image, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "    predicted_label = logits.argmax(-1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, Dinov2ForImageClassification\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "#dataset = load_dataset(\"huggingface/cats-image\")\n",
    "#image = dataset[\"test\"][\"image\"][0]\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
    "model = Dinov2ForImageClassification.from_pretrained(\"facebook/dinov2-base\")\n",
    "\n",
    "for image in \n",
    "\n",
    "    inputs = image_processor(image, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "    predicted_label = logits.argmax(-1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
